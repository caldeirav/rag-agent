{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Agent with llama stack: Advanced Agent Capabilities with Prompt Chaining and ReAct Agent\n",
    "\n",
    "This tutorial showcases how to build a local agent leveraging the llama stack, introducing techniques that make a simple retrieval agent smarter and more autonomous: **Prompt Chaining** and the **ReAct (Reasoning + Acting) framework**. These approaches allow the agent to complete multi-step tasks, dynamically choose tools, and adjust its behavior based on context.\n",
    "\n",
    "- **Prompt Chaining** connects multiple prompts into a coherent sequence, allowing the agent to maintain context and perform multi-step reasoning across tool invocations. \n",
    "- **ReAct Agent** combines reasoning and acting steps in a loop, enabling the agent to make decisions, use tools dynamically, and adapt based on intermediate results. \n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we'll explore three agent configurations:\n",
    "1. **Simple Retrieval Agent (Baseline)** ‚Äì Uses a single web search tool.\n",
    "2. **Prompt Chaining** ‚Äì Performs structured, multi-step reasoning by chaining prompts and responses.\n",
    "3. **ReAct Agent** ‚Äì Dynamically plans and executes actions using a loop of reasoning and tool use.\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting this notebook, ensure that you have:\n",
    "- Access to a [Llama Stack](https://llama-stack.readthedocs.io/en/latest/) server.\n",
    "- A Tavily API key. It is critical for this notebook to run correctly. You can register for one at [https://tavily.com/](https://tavily.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting Up this Notebook\n",
    "We will start with a few imports needed for this demo only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client import Agent, AgentEventLogger\n",
    "from llama_stack_client.lib.agents.event_logger import EventLogger\n",
    "from llama_stack_client.lib.agents.react.agent import ReActAgent\n",
    "from llama_stack_client.lib.agents.react.tool_parser import ReActOutput\n",
    "import sys\n",
    "sys.path.append('..') \n",
    "from src.client_tools import get_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will initialize our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Llama Stack server\n",
      "Inference Parameters:\n",
      "\tModel: granite3.2:latest\n",
      "\tSampling Parameters: {'strategy': {'type': 'greedy'}, 'max_tokens': 512}\n",
      "\tstream: False\n"
     ]
    }
   ],
   "source": [
    "# for accessing the environment variables\n",
    "# rename or copy the .env.example file to create a new file called .env\n",
    "# for this demo you will need the location of the Llama Stack server endpoint \n",
    "# and your personal Tavily api key for web search\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# for communication with Llama Stack\n",
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "# pretty print of the results returned from the model/agent\n",
    "import sys\n",
    "sys.path.append('..')  \n",
    "from src.utils import step_printer\n",
    "from termcolor import cprint\n",
    "\n",
    "base_url = os.getenv(\"REMOTE_BASE_URL\")\n",
    "\n",
    "# Tavily search API key is required for some of our demos and must be provided to the client upon initialization.\n",
    "# We will cover it in the agentic demos that use the respective tool.\n",
    "tavily_search_api_key = os.getenv(\"TAVILY_SEARCH_API_KEY\")\n",
    "if tavily_search_api_key is None:\n",
    "    provider_data = None\n",
    "else:\n",
    "    provider_data = {\"tavily_search_api_key\": tavily_search_api_key}\n",
    "\n",
    "client = LlamaStackClient(\n",
    "    base_url=base_url\n",
    ")\n",
    "    \n",
    "print(f\"Connected to Llama Stack server\")\n",
    "\n",
    "# model_id for the model you wish to use that is configured with the Llama Stack server\n",
    "model_id = \"granite3.2:latest\"\n",
    "\n",
    "temperature = float(os.getenv(\"TEMPERATURE\", 0.0))\n",
    "if temperature > 0.0:\n",
    "    top_p = float(os.getenv(\"TOP_P\", 0.95))\n",
    "    strategy = {\"type\": \"top_p\", \"temperature\": temperature, \"top_p\": top_p}\n",
    "else:\n",
    "    strategy = {\"type\": \"greedy\"}\n",
    "\n",
    "max_tokens = int(os.getenv(\"MAX_TOKENS\", 4096))\n",
    "\n",
    "# sampling_params will later be used to pass the parameters to Llama Stack Agents/Inference APIs\n",
    "sampling_params = {\n",
    "    \"strategy\": strategy,\n",
    "    \"max_tokens\": max_tokens,\n",
    "}\n",
    "\n",
    "stream_env = os.getenv(\"STREAM\", \"False\")\n",
    "# the Boolean 'stream' parameter will later be passed to Llama Stack Agents/Inference APIs\n",
    "# any value non equal to 'False' will be considered as 'True'\n",
    "stream = (stream_env != \"False\")\n",
    "\n",
    "print(f\"Inference Parameters:\\n\\tModel: {model_id}\\n\\tSampling Parameters: {sampling_params}\\n\\tstream: {stream}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure an Agent for tool use (Baseline)\n",
    "First we create an Agent instance with the desired LLM model, agent instructions and tools.\n",
    "\n",
    "Instructions: The instructions parameter, also referred to as the system prompt, specifies the agent's role and behavior. In this example, the agent is configured as a helpful web search assistant. It is instructed to use a tool whenever a web search is required and to respond in a friendly and helpful tone.\n",
    "\n",
    "Tools: The tools parameter defines the tools available to the agent. In this case, the builtin::websearch tool is used, which enables the agent to perform web searches. This tool is essential for retrieving up-to-date information from the web.\n",
    "\n",
    "How It Works: When a user query is provided, the agent processes the input and determines whether a tool is required to fulfill the request. If the query involves retrieving information from the web, the agent invokes the builtin::websearch tool. The tool interacts with Tavily Search to fetch real-time data, which is then processed and returned to the user in a friendly and helpful tone. This workflow ensures that the agent can handle a wide range of queries effectively.\n",
    "\n",
    "For more details on the builtin::websearch tool and its capabilities, refer to the [Llama-stack tools documentation](https://llama-stack.readthedocs.io/en/latest/building_applications/tools.html#web-search-providers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created session_id=8bddd29c-3342-4154-a805-6fdce75d0232 for Agent(0ca512c1-b4bc-4d84-b17c-6e8545a2c0b3)\n",
      "\n",
      "==================================================\n",
      "\u001b[34mProcessing user query: Are there any immediate weather-related risks in my area that could disrupt network connectivity or system availability?\u001b[0m\n",
      "==================================================\n",
      "\n",
      "---------- üìç Step 1: InferenceStep ----------\n",
      "ü§ñ Model Response:\n",
      "\u001b[35mI'm sorry for the inconvenience, but as an AI, I don't have real-time capabilities to access your current location or check local weather updates. However, you can quickly find this information by using a reliable weather forecasting service or app. Websites like the National Weather Service (for US), Met Office (for UK), or similar services in your region should provide accurate and up-to-date weather information.\n",
      "\u001b[0m\n",
      "========== Query processing completed ========== \n",
      "\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(\n",
    "    client, \n",
    "    model=model_id,\n",
    "    instructions=\"\"\"You are a helpful websearch assistant. When you are asked to search the latest you must use a tool. \n",
    "            Whenever a tool is called, be sure return the response in a friendly and helpful tone.\n",
    "            \"\"\" ,\n",
    "    tools=[\"builtin::websearch\"],\n",
    "    sampling_params=sampling_params\n",
    ")\n",
    "\n",
    "session_id = agent.create_session(\"web-session\")\n",
    "print(f\"Created session_id={session_id} for Agent({agent.agent_id})\")\n",
    "\n",
    "user_prompts = [\n",
    "    \"Are there any immediate weather-related risks in my area that could disrupt network connectivity or system availability?\",\n",
    "]\n",
    "\n",
    "for prompt in user_prompts:\n",
    "    print(\"\\n\"+\"=\"*50)\n",
    "    cprint(f\"Processing user query: {prompt}\", \"blue\")\n",
    "    print(\"=\"*50)\n",
    "    response = agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        session_id=session_id,\n",
    "    )\n",
    "\n",
    "    if stream:\n",
    "        for log in EventLogger().log(response):\n",
    "            log.print()\n",
    "    else:\n",
    "        step_printer(response.steps) # print the steps of an agent's response in a formatted way. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Analysis\n",
    "\n",
    "In this example, since the agent is unaware of the users location, it hallucinates one and generates an incorrect search query. This misidentification leads to inaccurate information about potential weather-related risks.\n",
    "\n",
    "This is where Prompt Chaining comes in. Prompt chaining allows the agent to:\n",
    "1. Maintain context across multiple queries\n",
    "2. Chain multiple tools together\n",
    "3. Use previous interactions to inform current decisions\n",
    "\n",
    "Let‚Äôs see how prompt chaining can improve the accuracy of the response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prompt chaining with websearch tool and client tool\n",
    "\n",
    "In this section, we demonstrate a more sophisticated use case that combines the use of two tools: location detection and web search.\n",
    "\n",
    "1. **Automatic Location Detection**: Use the `get_location` client tool to automatically determine the user's current location.\n",
    "2. **Contextual Search**: Leverage the detected location to formulate the correct websearch query.\n",
    "\n",
    "For example, when a user asks \"Are there any weather-related risks in my area that could disrupt network connectivity or system availability?\", the agent will:\n",
    "- First detect the user's current location using `get_location`.\n",
    "- Then use that location to search for nearby weather-related risks.\n",
    "- Finally, present a comprehensive response.\n",
    "\n",
    "This demonstrates how the builtin websearch tool and custom client tools can work together to provide intelligent, context-aware responses without requiring explicit location input from the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "\u001b[34mProcessing user query: Where am I?\u001b[0m\n",
      "==================================================\n",
      "\n",
      "---------- üìç Step 1: InferenceStep ----------\n",
      "ü§ñ Model Response:\n",
      "\u001b[35m<tool_call>[{\"name\": \"get_location\", \"arguments\": {}}]\n",
      "\u001b[0m\n",
      "========== Query processing completed ========== \n",
      "\n",
      "\n",
      "==================================================\n",
      "\u001b[34mProcessing user query: Are there any immediate weather-related risks in my area that could disrupt network connectivity or system availability?\u001b[0m\n",
      "==================================================\n",
      "\n",
      "---------- üìç Step 1: InferenceStep ----------\n",
      "ü§ñ Model Response:\n",
      "\u001b[35m<tool_call>[{\"name\": \"websearch\", \"arguments\": {\"query\": \"weather alerts near me\"}}]\n",
      "\u001b[0m\n",
      "========== Query processing completed ========== \n",
      "\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(\n",
    "    client, \n",
    "    model=model_id,\n",
    "    instructions=\"\"\"You are a helpful assistant. \n",
    "    When a user asks about their location, you MUST use the get_location tool. When you are asked to search the latest news, you MUST use the websearch tool.\n",
    "    \"\"\" ,\n",
    "    tools=[get_location, \"builtin::websearch\"],\n",
    "    sampling_params=sampling_params\n",
    ")\n",
    "user_prompts = [\n",
    "    \"Where am I?\",\n",
    "    \"Are there any immediate weather-related risks in my area that could disrupt network connectivity or system availability?\"\n",
    "]\n",
    "session_id = agent.create_session(\"prompt-chaining-session\")  # for prompt chaining, queries must share the same session_id.\n",
    "for prompt in user_prompts:\n",
    "    print(\"\\n\"+\"=\"*50)\n",
    "    cprint(f\"Processing user query: {prompt}\", \"blue\")\n",
    "    print(\"=\"*50)\n",
    "    response = agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        session_id=session_id,\n",
    "        stream=stream\n",
    "    )\n",
    "\n",
    "    if stream:\n",
    "        for log in EventLogger().log(response):\n",
    "            log.print()\n",
    "    else:\n",
    "        step_printer(response.steps) # print the steps of an agent's response in a formatted way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReAct Agent with websearch tool and client tool\n",
    "\n",
    "This section demonstrates the ReAct (Reasoning and Acting) framework in action.\n",
    "\n",
    "Here is a walkthrough of how the ReAct agent will tackle this same \"weather near me\" problem:\n",
    "\n",
    "When asked \"Are there any weather-related risks in my area that could disrupt network connectivity or system availability?\", the agent will:\n",
    "\n",
    "1. **Reason** that it needs to get location information first.\n",
    "2. **Act** by calling the `get_location` client tool.\n",
    "3. **Observe** the location result.\n",
    "4. **Reason** that it now needs to search for weather in that location.\n",
    "5. **Act** by calling the `websearch` tool with observed location.\n",
    "6. **Observe** and processes the search results into a final answer. \n",
    "\n",
    "Unlike prompt chaining which follows fixed steps, ReAct dynamically breaks down tasks and adapts its approach based on the results of each step. This makes it more flexible and capable of handling complex, real-world queries effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "\u001b[34mProcessing user query: Are there any immediate weather-related risks in my area that could disrupt network connectivity or system availability?\u001b[0m\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'detail': 'Invalid value: Pass Search provider\\'s API Key in the header X-LlamaStack-Provider-Data as { \"tavily_search_api_key\": <your api key>}'}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m cprint(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessing user query: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mblue\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m50\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m response = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_turn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43msession_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43msession_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[32m     30\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m log \u001b[38;5;129;01min\u001b[39;00m EventLogger().log(response):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/rag-agent/.venv/lib/python3.11/site-packages/llama_stack_client/lib/agents/agent.py:257\u001b[39m, in \u001b[36mAgent.create_turn\u001b[39m\u001b[34m(self, messages, session_id, toolgroups, documents, stream)\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_turn_streaming(messages, session_id, toolgroups, documents)\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m     chunks = \u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_turn_streaming\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoolgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    258\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunks:\n\u001b[32m    259\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTurn did not complete\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/rag-agent/.venv/lib/python3.11/site-packages/llama_stack_client/lib/agents/agent.py:257\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_turn_streaming(messages, session_id, toolgroups, documents)\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m     chunks = \u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_turn_streaming\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoolgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    258\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunks:\n\u001b[32m    259\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTurn did not complete\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/rag-agent/.venv/lib/python3.11/site-packages/llama_stack_client/lib/agents/agent.py:318\u001b[39m, in \u001b[36mAgent._create_turn_streaming\u001b[39m\u001b[34m(self, messages, session_id, toolgroups, documents)\u001b[39m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[32m    317\u001b[39m \u001b[38;5;66;03m# run the tools\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m318\u001b[39m tool_responses = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_tool_calls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool_calls\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[38;5;66;03m# pass it to next iteration\u001b[39;00m\n\u001b[32m    321\u001b[39m turn_response = \u001b[38;5;28mself\u001b[39m.client.agents.turn.resume(\n\u001b[32m    322\u001b[39m     agent_id=\u001b[38;5;28mself\u001b[39m.agent_id,\n\u001b[32m    323\u001b[39m     session_id=session_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.session_id[-\u001b[32m1\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    326\u001b[39m     stream=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    327\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/rag-agent/.venv/lib/python3.11/site-packages/llama_stack_client/lib/agents/agent.py:206\u001b[39m, in \u001b[36mAgent._run_tool_calls\u001b[39m\u001b[34m(self, tool_calls)\u001b[39m\n\u001b[32m    204\u001b[39m responses = []\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tool_call \u001b[38;5;129;01min\u001b[39;00m tool_calls:\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m     responses.append(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_single_tool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool_call\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m responses\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/rag-agent/.venv/lib/python3.11/site-packages/llama_stack_client/lib/agents/agent.py:229\u001b[39m, in \u001b[36mAgent._run_single_tool\u001b[39m\u001b[34m(self, tool_call)\u001b[39m\n\u001b[32m    227\u001b[39m \u001b[38;5;66;03m# builtin tools executed by tool_runtime\u001b[39;00m\n\u001b[32m    228\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tool_call.tool_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_tools:\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m     tool_result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtool_runtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke_tool\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtool_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtool_call\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtool_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtool_call\u001b[49m\u001b[43m.\u001b[49m\u001b[43marguments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbuiltin_tools\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtool_call\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtool_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ToolResponseParam(\n\u001b[32m    234\u001b[39m         call_id=tool_call.call_id,\n\u001b[32m    235\u001b[39m         tool_name=tool_call.tool_name,\n\u001b[32m    236\u001b[39m         content=tool_result.content,\n\u001b[32m    237\u001b[39m     )\n\u001b[32m    239\u001b[39m \u001b[38;5;66;03m# cannot find tools\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/rag-agent/.venv/lib/python3.11/site-packages/llama_stack_client/resources/tool_runtime/tool_runtime.py:84\u001b[39m, in \u001b[36mToolRuntimeResource.invoke_tool\u001b[39m\u001b[34m(self, kwargs, tool_name, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke_tool\u001b[39m(\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     62\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m     71\u001b[39m ) -> ToolInvocationResult:\n\u001b[32m     72\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     73\u001b[39m \u001b[33;03m    Run a tool with the given arguments\u001b[39;00m\n\u001b[32m     74\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     82\u001b[39m \u001b[33;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/v1/tool-runtime/invoke\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mkwargs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtool_runtime_invoke_tool_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mToolRuntimeInvokeToolParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mToolInvocationResult\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/rag-agent/.venv/lib/python3.11/site-packages/llama_stack_client/_base_client.py:1222\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1208\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1209\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1210\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1217\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1218\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1219\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1220\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1221\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1222\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/rag-agent/.venv/lib/python3.11/site-packages/llama_stack_client/_base_client.py:1031\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1028\u001b[39m             err.response.read()\n\u001b[32m   1030\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1031\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1033\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1035\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'detail': 'Invalid value: Pass Search provider\\'s API Key in the header X-LlamaStack-Provider-Data as { \"tavily_search_api_key\": <your api key>}'}"
     ]
    }
   ],
   "source": [
    "agent = ReActAgent(\n",
    "            client=client,\n",
    "            model=model_id,\n",
    "            tools=[get_location, \"builtin::websearch\"],\n",
    "            response_format={\n",
    "                \"type\": \"json_schema\",\n",
    "                \"json_schema\": ReActOutput.model_json_schema(),\n",
    "            },\n",
    "            sampling_params=sampling_params,\n",
    "        )\n",
    "user_prompts = [\n",
    "    \"Are there any immediate weather-related risks in my area that could disrupt network connectivity or system availability?\"\n",
    "]\n",
    "session_id = agent.create_session(\"web-session\")\n",
    "for prompt in user_prompts:\n",
    "    print(\"\\n\"+\"=\"*50)\n",
    "    cprint(f\"Processing user query: {prompt}\", \"blue\")\n",
    "    print(\"=\"*50)\n",
    "    response = agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        session_id=session_id,\n",
    "        stream=stream\n",
    "    )\n",
    "    if stream:\n",
    "        for log in EventLogger().log(response):\n",
    "            log.print()\n",
    "    else:\n",
    "        step_printer(response.steps) # print the steps of an agent's response in a formatted way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "- This notebook demonstrated how to build more capable agents using Prompt Chaining and the ReAct framework.\n",
    "- It showed how agents can maintain context across multiple steps and perform structured, multi-step reasoning.\n",
    "- It highlights how ReAct enables dynamic tool selection and adaptive decision-making based on intermediate results.\n",
    "- These techniques enhance agent autonomy and make them more suitable for complex operational tasks.\n",
    "\n",
    "For further extensions, continue exploring in the next notebook: [RAG Agents](Level4_RAG_agent.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Any Feedback?\n",
    "\n",
    "If you have any feedback on this or any other notebook in this demo series we'd love to hear it! Please go to https://www.feedback.redhat.com/jfe/form/SV_8pQsoy0U9Ccqsvk and help us improve our demos. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
